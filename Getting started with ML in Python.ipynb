{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Getting started with ML in Python  \n",
    "Welcome to your (perhaps!) first introduction to Machine Learning in Python! Python is the language of choice when it comes to Machine Learning because of the amount resources, libraries and community. `Scikit-learn` (imported as `sklearn`) is a widely used Python library for machine learning, and it offers a straightforward way to implement machine learning algorithms. Like most widely used libraries in Python, it has great documentation and `sklearn` format is what most developers tend to follow due to how commonly used it is!  \n",
    "\n",
    "We're going to kick things off with some Supervised Machine Learning and what better way than to revisit our old pal: Linear Regression! We will also be seeing another form of supervised learning algorithm called Decision Trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## Let's import our libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split # for creating subsets of our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Supervised Learning  \n",
    "Recall that supervised learning is where we have access to the ground truth labels and we can use them to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression # our model\n",
    "from sklearn.tree import DecisionTreeRegressor # our model\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## Let's generate some random data to work with \n",
    "\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) # this is our target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "24c3ddde1448de0186e70388b7a6fcdeff7867b4",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 411,
       "width": 703
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (100, 1)\n",
      "y (100, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X:', X.shape)\n",
    "print('y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember, that before we train the models, we need to split the data into training and testing sets. This allows you to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## Implement Linear Regression!\n",
    "\n",
    "# Create a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_linear = linear_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's talk about Decision Trees  \n",
    "Let's break down Decision Trees in simple terms:\n",
    "\n",
    "**What is a Decision Tree?**\n",
    "\n",
    "A Decision Tree is a flowchart-like structure used for making decisions or classifying items. In the context of machine learning, a Decision Tree is a tree-like model that represents decisions and their possible consequences, including the final decision or outcome.\n",
    "\n",
    "**Components of a Decision Tree:**\n",
    "\n",
    "1. **Root Node:** The top-most node in the tree, which represents the starting point of the decision-making process.\n",
    "\n",
    "2. **Internal Nodes:** These nodes represent decisions or tests on a particular attribute or feature. They have branches that lead to other nodes.\n",
    "\n",
    "3. **Branches:** The lines connecting nodes, which represent the possible outcomes of a decision.\n",
    "\n",
    "4. **Leaves (Terminal Nodes):** These are the endpoints of the tree, where a final decision or classification is made.\n",
    "\n",
    "**How Decision Trees Work:**\n",
    "\n",
    "1. **Choosing the Best Split:** Decision Trees work by repeatedly making binary decisions, which means they split the data into two parts at each internal node. The goal is to find the best feature and the best value to split the data. This decision is based on criteria that aim to maximize the separation or purity of data at each split. Common criteria include Gini impurity and entropy for classification tasks and mean squared error for regression tasks.\n",
    "\n",
    "2. **Recursive Splitting:** Decision Trees continue to split the data into smaller and smaller subsets until a stopping condition is met. This condition could be a predefined depth of the tree, a minimum number of samples per leaf, or other criteria.\n",
    "\n",
    "3. **Classifying or Predicting:** Once the tree is built, it can be used to classify new data points (for classification tasks) or make predictions (for regression tasks). The data point is passed down the tree, following the decisions made at each internal node, until it reaches a leaf node, which provides the final decision or prediction.\n",
    "\n",
    "**Advantages of Decision Trees:**\n",
    "\n",
    "- Easy to understand and interpret.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Suitable for a wide range of tasks, including classification and regression.\n",
    "- Can be used for feature selection, as they provide information about feature importance.\n",
    "- Computationally efficient, and their prediction time is typically very fast.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Prone to overfitting if the tree is too deep or complex.\n",
    "- Sensitive to small variations in the data, which can lead to different tree structures.\n",
    "- Single decision trees may not always provide the highest accuracy compared to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## Let's implement a Decision Tree in sklearn!\n",
    "\n",
    "# Create a Decision Tree Regressor model\n",
    "tree_model = DecisionTreeRegressor(max_depth=3)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_tree = tree_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "2b9f7dc8213a917ddeddd26cdc4a679c861cff5f",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 394,
       "width": 683
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.plot_tree(tree_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is a related (and much more commonly used algorithm!) called **_Random Forest_**! You can read all about it [here](https://www.ibm.com/topics/random-forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have implemented our models and have created predictions - let's see how well we did! For this we will need a *loss function*! `sklearn` has many functions to choose from, here we will make use of the Mean Absolute Error (MAE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.7411599160566334\n",
      "Decision Tree MAE: 1.0522524514872227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
    "\n",
    "print(f\"Linear Regression MAE: {mae_linear}\")\n",
    "print(f\"Decision Tree MAE: {mae_tree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "e3245e37725dfe4aeb32d3839d71af7362052a7f",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 440,
       "width": 717
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also visualize the regression models and their predictions.\n",
    "\n",
    "plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "plt.plot(X_test, y_pred_linear, 'o', color='red', label=\"Linear Regression\")\n",
    "plt.plot(X_test, y_pred_tree, 'o', color='green', label=\"Decision Tree\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regression Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Unsupervised Learning  \n",
    "Unsupervised learning is a type of machine learning where the algorithm learns patterns and structures in data without explicit labels. So here, we cannot classify our datapoints explicitly, rather we *cluster* them. A cluster is a group of data points that more similar to one another than a data point in another cluster. We can choose our measure for 'similarity'.  \n",
    "\n",
    "A very popular unsupervised algorithm for clustering is called **K-Means Clustering**. Let's see how we can do this in `sklearn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.cluster import KMeans # import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "## Let's generate some new data \n",
    "\n",
    "data = np.random.rand(100, 2)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In K-Means, you need to specify the number of clusters (k) that the algorithm should discover in the data. \n",
    "# Choose a reasonable value for k. More on this later!\n",
    "\n",
    "k = 3\n",
    "\n",
    "# Let's fit the model!\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## The model has assigned each data point to one of the clusters. You can access the cluster labels as follows:\n",
    "\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "6e43e8ce06416b2f99fcaa650ce3c067d025687c",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 426,
       "width": 706
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's visualize the clusters!\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "53f1f9107e3d88ebfc5c20bad831b990eaa889e9",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 426,
       "width": 706
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You can also visualize the cluster centres (centroids)!\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=200)\n",
    "plt.title(\"K-Means Clustering with Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Note on determining the optimal k:** Choosing the right number of clusters (k) is important. You can use methods like [the elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) or [silhouette score](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) to determine the optimal value of k. Experiment with different values to see which one best fits your data. Let's see how we can use the silhouette score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Silhouette Score  \n",
    "The Silhouette Score is a metric used to measure the quality of clusters in K-Means clustering. It quantifies how similar an object is to its own cluster (cohesion) compared to other clusters (separation). A higher Silhouette Score indicates that the clusters are well-separated and appropriately grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=2, Silhouette Score = 0.37763905718541074\n",
      "For k=3, Silhouette Score = 0.38954952500321527\n",
      "For k=4, Silhouette Score = 0.4047826148032956\n",
      "For k=5, Silhouette Score = 0.387826591432025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=6, Silhouette Score = 0.3832392990345265\n",
      "For k=7, Silhouette Score = 0.36244426118428963\n",
      "For k=8, Silhouette Score = 0.38197358346476157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=9, Silhouette Score = 0.3428589711456768\n",
      "For k=10, Silhouette Score = 0.37922217330944474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "## Let's try a number of different values for k on our original data\n",
    "\n",
    "# Try k values from 2 to 10 (or a suitable range)\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(f\"For k={k}, Silhouette Score = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The best one seems to be `k=4`, so let's re-run our analysis from before and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "\n",
    "# Let's fit the model!\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(data)\n",
    "\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 3, 1, 3, 0, 1, 3, 1,\n",
       "       2, 0, 3, 3, 2, 2, 1, 3, 3, 2, 2, 3, 0, 3, 1, 2, 1, 2, 3, 1, 2, 2,\n",
       "       1, 1, 3, 3, 2, 0, 2, 2, 1, 2, 3, 0, 2, 3, 3, 1, 1, 1, 2, 0, 3, 2,\n",
       "       0, 1, 2, 3, 2, 0, 1, 3, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 0, 2, 3, 2,\n",
       "       3, 1, 0, 3, 1, 0, 1, 1, 1, 2, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "23d80cf4dbe4966056cd58afeebcbef2ca672950",
      "text/plain": "<Figure size 864x504 with 2 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 426,
       "width": 648
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_centers = kmeans.cluster_centers_\n",
    "plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=200)\n",
    "plt.colorbar()\n",
    "plt.title(\"K-Means Clustering with Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}